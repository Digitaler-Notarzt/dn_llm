{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "from datasets import load_dataset\n", "from transformers import (\n", "    AutoModelForCausalLM,\n", "    AutoTokenizer,\n", "    BitsAndBytesConfig,\n", "    TrainingArguments,\n", ")\n", "from trl import SFTTrainer\n", "from peft import LoraConfig\n", "import platform"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The model that you want to train from the Hugging Face hub"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_name = \"NousResearch/Llama-2-7b-chat-hf\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["The instruction dataset to use"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset_name = \"codyburker/yelp_review_sampled\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Fine-tuned model name"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_model = \"DINO\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["##############################################################################<br>\n", "QLoRA parameters<br>\n", "##############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["LoRA attention dimension"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lora_r = 32  # Reduced from 64"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Alpha parameter for LoRA scaling"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lora_alpha = 16"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Dropout probability for LoRA layers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lora_dropout = 0.1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##############################################################################<br>\n", "bitsandbytes parameters<br>\n", "##############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Activate 4-bit precision base model loading"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["use_4bit = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Compute dtype for 4-bit base models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bnb_4bit_compute_dtype = \"float16\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Quantization type (fp4 or nf4)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bnb_4bit_quant_type = \"nf4\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Activate nested quantization for 4-bit base models (double quantization)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["use_nested_quant = True  # Changed to True for further memory savings"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##############################################################################<br>\n", "TrainingArguments parameters<br>\n", "##############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Output directory where the model predictions and checkpoints will be stored"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output_dir = \"./results\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Number of training epochs"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_train_epochs = 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enable fp16/bf16 training (set bf16 to True with an A100)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fp16 = True  # Changed to True\n", "bf16 = False  # Changed to False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Batch size per GPU for training"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["per_device_train_batch_size = 1  # Reduced from 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Batch size per GPU for evaluation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["per_device_eval_batch_size = 1  # Reduced from 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Number of update steps to accumulate the gradients for"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gradient_accumulation_steps = 4  # Increased from 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Enable gradient checkpointing"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["gradient_checkpointing = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Maximum gradient normal (gradient clipping)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_grad_norm = 0.3"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initial learning rate (AdamW optimizer)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["learning_rate = 1e-4  # Reduced from 2e-4"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Weight decay to apply to all layers except bias/LayerNorm weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["weight_decay = 0.001"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optimizer to use"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["optim = \"paged_adamw_32bit\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Learning rate schedule (constant a bit better than cosine)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lr_scheduler_type = \"constant\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Number of training steps (overrides num_train_epochs)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_steps = 100  # Added a limit to reduce overall training time"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ratio of steps for a linear warmup (from 0 to learning rate)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["warmup_ratio = 0.03"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Group sequences into batches with same length<br>\n", "Saves memory and speeds up training considerably"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["group_by_length = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save checkpoint every X updates steps"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["save_steps = 50  # Increased from 25"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Log every X updates steps"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["logging_steps = 50  # Increased from 25"]}, {"cell_type": "markdown", "metadata": {}, "source": ["##############################################################################<br>\n", "SFT parameters<br>\n", "##############################################################################"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Maximum sequence length to use"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_seq_length = 512  # Added a limit to reduce memory usage"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Pack multiple short examples in the same input sequence to increase efficiency"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["packing = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the entire model on the GPU 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device_map = {\"\": 0}\n", "mac_device_map = {\"mps\": 0}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load dataset (you can process it here)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = load_dataset(dataset_name, split=\"train\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load tokenizer and model with QLoRA configuration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bnb_config = BitsAndBytesConfig(\n", "    load_in_4bit=use_4bit,\n", "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n", "    bnb_4bit_compute_dtype=compute_dtype,\n", "    bnb_4bit_use_double_quant=use_nested_quant,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if(not platform.system() == \"Darwin\"):\n", "    # Check GPU compatibility with bfloat16\n", "    if compute_dtype == torch.float16 and use_4bit:\n", "        major, _ = torch.cuda.get_device_capability()\n", "        if major >= 8:\n", "            print(\"=\" * 80)\n", "            print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n", "            print(\"=\" * 80)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load base model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    quantization_config=bnb_config,\n", "    device_map=device_map,\n", "    max_memory={0:\"2GB\",\"cpu\": \"30GiB\"},  # Reduced memory allocation\n", ")\n", "model.config.use_cache = False  # Changed to False to save memory\n", "model.config.pretraining_tp = 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load LLaMA tokenizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load LoRA configuration"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["peft_config = LoraConfig(\n", "    lora_alpha=lora_alpha,\n", "    lora_dropout=lora_dropout,\n", "    r=lora_r,\n", "    bias=\"none\",\n", "    task_type=\"CAUSAL_LM\",\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set training parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training_arguments = TrainingArguments(\n", "    output_dir=output_dir,\n", "    num_train_epochs=num_train_epochs,\n", "    per_device_train_batch_size=per_device_train_batch_size,\n", "    gradient_accumulation_steps=gradient_accumulation_steps,\n", "    optim=optim,\n", "    save_steps=save_steps,\n", "    logging_steps=logging_steps,\n", "    learning_rate=learning_rate,\n", "    weight_decay=weight_decay,\n", "    fp16=fp16,\n", "    bf16=bf16,\n", "    max_grad_norm=max_grad_norm,\n", "    max_steps=max_steps,\n", "    warmup_ratio=warmup_ratio,\n", "    group_by_length=group_by_length,\n", "    lr_scheduler_type=lr_scheduler_type,\n", "    report_to=\"tensorboard\"\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Set supervised fine-tuning parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer = SFTTrainer(\n", "    model=model,\n", "    train_dataset=dataset,\n", "    peft_config=peft_config,\n", "    dataset_text_field=\"text\",\n", "    max_seq_length=max_seq_length,\n", "    tokenizer=tokenizer,\n", "    args=training_arguments,\n", "    packing=packing,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer.train()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Save trained model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer.model.save_pretrained(new_model)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}