{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os\n", "import torch\n", "from datasets import load_dataset\n", "from transformers import (\n", "    AutoModelForCausalLM,\n", "    AutoTokenizer,\n", "    BitsAndBytesConfig,\n", "    TrainingArguments,\n", ")\n", "from trl import SFTTrainer\n", "from peft import LoraConfig\n", "import platform"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n", "dataset_name = \"codyburker/yelp_review_sampled\"\n", "new_model = \"DINO\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lora_r = 32\n", "lora_alpha = 16\n", "lora_dropout = 0.1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["use_4bit = True\n", "bnb_4bit_compute_dtype = \"float16\"\n", "bnb_4bit_quant_type = \"nf4\"\n", "use_nested_quant = True"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["output_dir = \"./results\"\n", "num_train_epochs = 1\n", "fp16 = True\n", "bf16 = False\n", "per_device_train_batch_size = 1\n", "per_device_eval_batch_size = 1\n", "gradient_accumulation_steps = 4\n", "gradient_checkpointing = True\n", "max_grad_norm = 0.3\n", "learning_rate = 1e-4\n", "weight_decay = 0.001\n", "optim = \"paged_adamw_32bit\"\n", "lr_scheduler_type = \"constant\"\n", "max_steps = 100\n", "warmup_ratio = 0.03\n", "group_by_length = True\n", "save_steps = 50\n", "logging_steps = 50"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["max_seq_length = 512\n", "packing = False\n", "device_map = {\"\": 0}\n", "mac_device_map = {\"mps\": 0}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = load_dataset(dataset_name, split=\"train\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["compute_dtype = getattr(torch, bnb_4bit_compute_dtype)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bnb_config = BitsAndBytesConfig(\n", "    load_in_4bit=use_4bit,\n", "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n", "    bnb_4bit_compute_dtype=compute_dtype,\n", "    bnb_4bit_use_double_quant=use_nested_quant,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if not platform.system() == \"Darwin\":\n", "    if compute_dtype == torch.float16 and use_4bit:\n", "        major, _ = torch.cuda.get_device_capability()\n", "        if major >= 8:\n", "            print(\"=\" * 80)\n", "            print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n", "            print(\"=\" * 80)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    quantization_config=bnb_config,\n", "    device_map=device_map,\n", "    max_memory={0: \"2GB\", \"cpu\": \"30GiB\"},\n", ")\n", "model.config.use_cache = False\n", "model.config.pretraining_tp = 1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"right\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["peft_config = LoraConfig(\n", "    lora_alpha=lora_alpha,\n", "    lora_dropout=lora_dropout,\n", "    r=lora_r,\n", "    bias=\"none\",\n", "    task_type=\"CAUSAL_LM\",\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["training_arguments = TrainingArguments(\n", "    output_dir=output_dir,\n", "    num_train_epochs=num_train_epochs,\n", "    per_device_train_batch_size=per_device_train_batch_size,\n", "    gradient_accumulation_steps=gradient_accumulation_steps,\n", "    optim=optim,\n", "    save_steps=save_steps,\n", "    logging_steps=logging_steps,\n", "    learning_rate=learning_rate,\n", "    weight_decay=weight_decay,\n", "    fp16=fp16,\n", "    bf16=bf16,\n", "    max_grad_norm=max_grad_norm,\n", "    max_steps=max_steps,\n", "    warmup_ratio=warmup_ratio,\n", "    group_by_length=group_by_length,\n", "    lr_scheduler_type=lr_scheduler_type,\n", "    report_to=\"tensorboard\"\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer = SFTTrainer(\n", "    model=model,\n", "    train_dataset=dataset,\n", "    peft_config=peft_config,\n", "    dataset_text_field=\"text\",\n", "    max_seq_length=max_seq_length,\n", "    tokenizer=tokenizer,\n", "    args=training_arguments,\n", "    packing=packing,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["trainer.train()\n", "trainer.model.save_pretrained(new_model)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}